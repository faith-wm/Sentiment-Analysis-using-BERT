{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    words_filtered = [e.lower() for e in text.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered if 'http' not in word\n",
    "                     and not word.startswith('@')\n",
    "                     and not word.startswith('#')\n",
    "                     and word != 'RT']\n",
    "    words_without_stopwords = [stemmer.stem(word) for word in words_cleaned if word not in stopwords_set]\n",
    "    return ' '.join(words_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color = 'black'):\n",
    "    data=' '.join(data)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(data)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud of al tweets\n",
    "wordcloud_draw(data['text'],'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive tweets\n",
    "data_positive = data[ data['sentiment'] == 'Positive']\n",
    "data_positive = data_positive['text']\n",
    "wordcloud_draw(data_positive,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative tweets\n",
    "data_negative = data[ data['sentiment'] == 'Negative']\n",
    "data_negative = data_negative['text']\n",
    "wordcloud_draw(data_negative,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label distribution\n",
    "data['sentiment'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class names to numbers\n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "data['sentiment'] = data['sentiment'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation,  test split\n",
    "train_df, test_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "test_df, valid_df = train_test_split(test_df, test_size=0.3, random_state=42) \n",
    "\n",
    "print(\"train size:\", len(train_df))\n",
    "print(\"validation size:\", len(valid_df))\n",
    "print(\"test size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pretrained_model = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in data['text']:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 120])\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model, do_lower_case=True)\n",
    "\n",
    "def encode(docs):\n",
    "\n",
    "    #  takes list of texts, returns input_ids and attention_masks\n",
    "    \n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                            return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n",
    "valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())\n",
    "test_input_ids, test_att_masks = encode(test_df['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch data loader\n",
    "train_y = torch.LongTensor(train_df['sentiment'].values.tolist())\n",
    "valid_y = torch.LongTensor(valid_df['sentiment'].values.tolist())\n",
    "test_y = torch.LongTensor(test_df['sentiment'].values.tolist())\n",
    "\n",
    "train_y.size(),valid_y.size(),test_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 16\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(train_df['sentiment'].unique())\n",
    "model = BertForSequenceClassification.from_pretrained(pretrained_model,\n",
    "                                                      num_labels=num_labels,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 3\n",
    "lr = 2e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch+1}')\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='training')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "        \n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        del loss\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss / (step_num + 1))              \n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "            loss = output.loss\n",
    "            valid_loss += loss.item()\n",
    "   \n",
    "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
    "    valid_pred = np.concatenate(valid_pred)\n",
    "    \n",
    "    print(f\"{step_num+1}/{math.ceil(len(train_df) / batch_size)} train loss: {train_loss / (step_num + 1)}\")\n",
    "    print(f\"{step_num_e+1}/{math.ceil(len(test_df) / batch_size)} val loss: {valid_loss / (step_num_e + 1)}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, epochs +1 )\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classification report')\n",
    "print(classification_report(valid_pred, valid_df['sentiment'].to_numpy(), target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels=None):\n",
    "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "  fig, ax = plt.subplots(figsize=(6, 6))\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) \n",
    "  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False) \n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.show()\n",
    "  \n",
    "plot_confusion_matrix(valid_pred,valid_df['sentiment'].to_numpy(),labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_pred = []\n",
    "test_loss= 0\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        test_loss += loss.item()\n",
    "   \n",
    "        test_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "test_pred = np.concatenate(test_pred)\n",
    "\n",
    "print(classification_report(test_pred, test_df['sentiment'].to_numpy(),target_names=class_names))\n",
    "plot_confusion_matrix(test_pred,test_df['sentiment'].to_numpy(),labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pred'] = test_pred\n",
    "test_df.reset_index(level=0)\n",
    "print(test_df[test_df['sentiment']!=test_df['pred']].shape)\n",
    "test_df[test_df['sentiment']!=test_df['pred']][['text','sentiment','pred']].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-vPnuWEC2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
